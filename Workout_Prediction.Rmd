---
title: "Workout Prediction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,message=FALSE)
library(caret)
```

## Intro and Data Prep

In this document, we will look at data collected by wearable fitness devices while the user was doing biceps curls - either the proper way, or wrong in one of 4 ways. This yields 5 different classes, with class A being the correct execution (saved in the "classe"" variable). 

We now first load the data:

```{r, cache=TRUE}
rest<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
quiztesting<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```
### Cleaning the Data
Looking at the dataset in the viewer, a lot of columns have many missing entries, either denoted by nothing or NA. We first convert the entries with nothing into NAs, then remove those columns that have more than half (9811) of their entries missing. In columns with less than 9811 entries missing, we will impute the entries.

```{r}
rest[rest==""]=NA
nalist<-sapply(rest,function(x) sum(is.na(x)))
cleaned<-rest[,nalist<9811]
sum(is.na(cleaned))
```

As it turns out, there no NAs left, so we don't need to impute. 

We will also kick out the timestamp-related data (since we are not treating it as a time series) and the index variable $X$. The latter is particularly important: It ranges from 1 to 19622 and is simply an enumeration of the data points. However, the data points are also sorted with respect to class, as can be seen in the Figure below:
```{r}
plot(cleaned$X,cleaned$classe,xlab="X",ylab="Class")
```

Thus, this index is a great predictor even though it has nothing to do with the actual data. Since the test set for the quiz is also numbered from 1 to 20, keeping this variable would likely result in nearly all predictions being "A" (or Class 1 on the plot).
```{r}
cleaned<-cleaned[,-c(1,3,4,5)]
```
We are left with 56 variables.


We'll do the same for the downloaded quiz testing set:
```{r}
nalist<-sapply(rest,function(x) sum(is.na(x)))
quiztesting<-quiztesting[,nalist<9811]
quiztesting<-quiztesting[,-c(1,3,4,5)]
sum(is.na(quiztesting))
```

Since the testing set (that is used for the quiz) is really small, we will set aside a larger set to estimate the out of sample error of the final model. We will use cross-validation built into the algorithms, so we don't manually set up a validation dataset.

```{r}
inTrain<-createDataPartition(y=cleaned$classe,p=0.8,list=FALSE)
training<-cleaned[inTrain,]
testing<-cleaned[-inTrain,]
```

## Preprocessing 

We first observe that 56 (55 without the class) variables is a lot of observations. The idea is to run a principle component analysis to get only the relevant predictors. We will then later run several modeling algorithms on the principal components and combine them via a random forest in the end.

We start with the PCA, keeping as many predictors as necessary to explain 99% of the variance:

```{r}
preProc<-preProcess(training[,-56],method="pca",thresh=0.99)
preProc
comps<-predict(preProc,training[,-56])
```

Note that PCA can only handle numeric attributes, so the two factor variables "user_name" and "new_window" are also still in the mix (and who knows, maybe the measurements are user-specific!). We now add the class variable back into the data frame. We also make a second data frame for the models that can't handle non-numeric inputs.

```{r}
comps$class<-training$classe
compsnum<-comps[,-c(1,2)]
```

## Building the models

We now take our PCA output and feed it into several different Machine Learning algorithms, which we will combine via random forest in the end. Basically, we pick the most powerful algorithms we covered in class. We promised we would do cross validation in the algorithms (where available), so we'll specify how with the trainControl function. We would have liked to choose repeated k-fold cross validation (This is like regular k-fold cross validation, but repeated a specified number of times and averaged), but it takes forever to run on this PC, which already has an i7 Processor. Thus, we compromise and do regular k-fold cross validation with $K=10$.

```{r}
train_control <- trainControl(method="cv", number=10)
```

### Random Forest

Our first model will be a random forest, because it can handle the non-numeric variables well and we learned in class that it performs well overall.

```{r, cache=TRUE}
modrf<-train(class~.,data=comps,trControl=train_control,method="rf")
```


### Adaboost

This may seem weird because it's an algorithm that already combines different models, but if you recall the lecture, it can fit very unnatural boundaries to the classification areas. Thus, we think it's a good idea to include this as well. Unfortunately, the caret version only does 2-class adaboost, so we need to load the adabag package.

```{r, cache=TRUE}
library(adabag)
modada<-boosting(class~.,data=comps)
```

### Naive Bayes

We also include a Naive Bayes Classifier, this will do well if the principle components are nicely linearly separable. The caret version somehow doesn't work and produces a lot of warnings, so we will use an external package for this.

```{r, cache=TRUE}
require(naivebayes)
modnb<-naive_bayes(class~.,data=comps)
```

### Support Vector Machine

We didn't really cover this in class and only saw it in the quiz for this week, but I'm still going to include it. On a high level, the idea is to map data that may not be linearly separable into a higher-dimensional space where it may be separable, and do the classification there. Thus, this is like a counterpart for the Naive Bayes in that it will cover the cases where the data is not linearly seperable. In the case that it is, both predictors should hopefully agree and thus push the final outcome in the right direction.

```{r, cache=TRUE}
require(e1071)
modsv<-svm(class~.,data=compsnum,cross=10)
```

### Other Notes

It would have been interesting to include things like Neural Networks, or just every algorithm we have seen, but we also have to keep the computational complexity in mind. Building these models already takes quite some time, so we will stick to these four as our basis for the fnal prediction, as we feel we have most aspects that could arise covered.

## Combining the Outputs
Now we will combine the four models into one big model.

### Input for the Combiner

We now use the four models we developed to generate input for the last level of our model. Concretely, we predict the classes of our test data, and use a random forest to combine the individual predictors. 

```{r, cache=TRUE}
predrf<-predict(modrf,data=comps)
predada<-predict.boosting(modada,newdata=comps)
prednb<-predict(modnb,newdata=compsnum)
predsvm<-predict(modsv,data=compsnum)

dfpreds<-data.frame(predrf,predada=predada$class,prednb,predsvm,class=comps$class)
```

### Combining the Outputs
We now have the predictions from our four different models, which we now combine into the final prediction with a random forest (again using 10-fold cross validation):

```{r, cache=TRUE}
finalmod<-train(class~.,data=dfpreds,trControl=train_control,method="rf")
```

## Performance
We now test performance on our testing set. The first step is the principle component analysis:

```{r}
testpcas<-predict(preProc,testing[,-59])
testpcasnum<-testpcas[,-c(1,2)]
```

Then we run the four intermediate models on the output:

```{r, cache=TRUE}
testrf<-predict(modrf,testpcas)
testada<-predict.boosting(modada,newdata=testpcas)
testnb<-predict(modnb,newdata=testpcasnum)
testsvm<-predict(modsv,testpcasnum)

testdfpreds<-data.frame(predrf=testrf,predada=as.factor(testada$class),prednb=testnb,predsvm=testsvm,class=testing$classe)
```

Lastly, we apply the final random forest to the output:

```{r, cache=TRUE}
finalpred<-predict(finalmod,testdfpreds)
```

We calculate the the accuracy:

```{r, cache=TRUE}
sum(finalpred==testing$classe)/length(testing$classe)
```
```{r, cache=TRUE, echo=FALSE}
acc<-sum(finalpred==testing$classe)/length(testing$classe)
```
So we have `r acc` accuracy, which doesn't sound bad :)

## Test Set for the Quiz
I don't know whether this should be in here, but since I have to do it anyway, I'll just include it.

### Preprocess
We again first apply the PCA (the last variable is the assignment number instead of the class, but we don't include it in the PCA anyway):

```{r, cache=TRUE}
quizpcas<-predict(preProc,quiztesting[,-59])
quizpcasnum<-quizpcas[,-c(1,2)]
```

### Four Intermediate Models
We apply our four models:

```{r, cache=TRUE}
quizrf<-predict(modrf,quizpcas)
quizada<-predict.boosting(modada,newdata=quizpcas)
quiznb<-predict(modnb,quizpcasnum)
quizsvm<-predict(modsv,quizpcasnum)

quizdfpreds<-data.frame(predrf=quizrf,predada=as.factor(quizada$class),prednb=quiznb,predsvm=quizsvm)
```

### Final Prediction

We feed the data frame into our final model:

```{r, cache=TRUE}
quizfinalpred<-predict(finalmod,quizdfpreds)
```

And our predictions are:

```{r, cache=TRUE}
quizfinalpred
```